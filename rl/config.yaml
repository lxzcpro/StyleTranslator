# Model configuration
model:
  name: "Qwen/Qwen2.5-0.5B-Instruct"
  path: "Qwen/Qwen2.5-0.5B-Instruct"
  device: "auto"  # auto, cpu, cuda

# Training configuration
training:
  num_generations: 4  # Number of generations per input
  batch_size: 1
  num_epochs: 1
  max_length: 512
  learning_rate: 1.0e-5
  beta: 0.04  # GRPO KL regularization coefficient
  gamma: 1.0  # Reward discount factor

# Reward model configuration
reward:
  # Style reward model paths (use environment variables or relative paths)
  chinese_bert_path: 'models/berts/chinese_style_detector_final.ckpt'
  english_bert_path: 'models/berts/english_style_detector_final.ckpt'

  # Style types (must match model training classes)
  style_types: ["law", "literature", "news", "science"]

  # Reward weights (format: 1, semantic: 6, style: 4)
  format_reward_weight: 1
  semantic_reward_weight: 6
  style_reward_weight: 4

  # COMET semantic reward configuration
  comet_model: "wmt22-cometkiwi-da"
  comet_device: "cpu"  # cpu or cuda
  comet_path: null  # null to auto-download, or specify path

  # Mode toggle (true: use mock models for testing)
  test_mode: false

# Data configuration
data:
  train_file: "data/train/train_style.parquet"
  test_file: "data/test/test_style.parquet"
  max_train_samples: 1000
  max_test_samples: 100

# Output configuration
output:
  output_dir: "./outputs"
  logging_steps: 10
  save_steps: 100