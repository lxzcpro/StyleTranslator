# Main configuration for RL training
# Use Hydra to compose configs: python scripts/train_rl.py env=local reward=default

defaults:
  - env: local
  - reward: default
  - model: qwen_0.5b
  - _self_

# Training parameters
training:
  num_generations: 8
  batch_size: 1
  num_epochs: 1
  max_length: 1024
  learning_rate: 1e-4
  beta: 0.04  # GRPO KL regularization
  gamma: 1.0  # Reward discount factor
  gradient_accumulation_steps: 1

# Data paths
data:
  train_file: "rl/data/train/parquet/train_style.parquet"
  test_file: "rl/data/test/parquet/test_style.parquet"
  max_train_samples: 1000
  max_test_samples: 100

# Output configuration
output:
  output_dir: "./outputs"
  logging_steps: 10
  save_steps: 100
  experiment_name: "style_translation_rl"

# Hydra settings
hydra:
  run:
    dir: outputs/${output.experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${output.experiment_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
