#!/usr/bin/env python3
"""
å¥–åŠ±ä½“ç³»æµ‹è¯•è„šæœ¬
æµ‹è¯•å®Œæ•´çš„å¥–åŠ±å‡½æ•°ï¼šæ ¼å¼å¥–åŠ±ã€COMETè¯­ä¹‰å¥–åŠ±ã€é£æ ¼å¥–åŠ±
"""

import json
import logging
import numpy as np
from typing import Dict, List, Any
import sys
import os
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from reward.reward_manager import RewardManager
from reward.format_score import FormatReward

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_test_data(file_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data['data']
    except Exception as e:
        logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
        return []


def create_mock_responses(original_en: str, translations: Dict[str, str]) -> List[str]:
    """åˆ›å»ºç¬¦åˆæ ¼å¼è¦æ±‚çš„æ¨¡æ‹Ÿç¿»è¯‘ç»“æœ"""
    mock_responses = []
    
    # å››ç§ç¿»è¯‘ç»“æœï¼Œæ¯ç§éƒ½æ·»åŠ æ ¼å¼æ ‡ç­¾
    for translation_type, translation_text in translations.items():
        # æ·»åŠ æ€è€ƒè¿‡ç¨‹ï¼ˆç®€å•æ¨¡æ‹Ÿï¼‰å’Œç¿»è¯‘ç»“æœ
        mock_response = f"<think>æ€è€ƒè¿‡ç¨‹ï¼šå°†è‹±æ–‡å¥å­'{original_en[:30]}...'ç¿»è¯‘æˆä¸­æ–‡</think>\n"
        mock_response += f"<translate>{translation_text}"
        mock_responses.append(mock_response)
    
    return mock_responses


def print_reward_details(test_round: int, original_en: str, reference_zh: str, 
                        mock_responses: List[str], reward_results: Dict[str, Any]):
    """æ‰“å°è¯¦ç»†çš„å¥–åŠ±ä¿¡æ¯"""
    print(f"\n{'='*80}")
    print(f"ç¬¬ {test_round + 1} è½®æµ‹è¯•")
    print(f"{'='*80}")
    
    print(f"\nğŸ“„ åŸæ–‡ (è‹±æ–‡):")
    print(f"   {original_en}")
    
    print(f"\nğŸ“„ å‚è€ƒè¯‘æ–‡ (ä¸­æ–‡):")
    print(f"   {reference_zh}")
    
    print(f"\nğŸ¯ å››æ¡æ¨¡æ‹Ÿç¿»è¯‘ç»“æœåŠå¥–åŠ±åˆ†æ•°:")
    
    translation_types = ["correct_style_correct_meaning", 
                        "correct_style_wrong_meaning",
                        "wrong_style_correct_meaning", 
                        "wrong_style_wrong_meaning"]
    
    for i, (response, trans_type) in enumerate(zip(mock_responses, translation_types)):
        format_reward = reward_results['format_rewards'][i]
        semantic_reward = reward_results['semantic_rewards'][i]
        style_reward = reward_results['style_rewards'][i]
        total_reward = reward_results['total_rewards'][i]
        
        # æå–ç¿»è¯‘å†…å®¹
        format_reward_obj = FormatReward()
        translation_content = format_reward_obj.extract_translation(response)
        
        print(f"\n   ç¿»è¯‘ {i+1} ({trans_type}):")
        print(f"   å†…å®¹: {translation_content}")
        print(f"   æ ¼å¼å¥–åŠ±: {format_reward['total_reward']:.3f} (æœ‰æ•ˆ: {format_reward['format_valid']})")
        print(f"   è¯­ä¹‰å¥–åŠ±: {semantic_reward:.3f}")
        print(f"   é£æ ¼å¥–åŠ±: {style_reward['style_score']:.3f}")
        print(f"   æ€»å¥–åŠ±:   {total_reward:.3f}")
        
        if format_reward['error_message']:
            print(f"   æ ¼å¼é”™è¯¯: {format_reward['error_message']}")


def print_style_vectors(test_round: int, original_en: str, reference_zh: str, 
                       style_rewards: List[Dict[str, Any]], style_types: List[str]):
    """æ‰“å°é£æ ¼å‘é‡ä¿¡æ¯"""
    print(f"\nğŸ¨ é£æ ¼åˆ†æ (ç¬¬ {test_round + 1} è½®):")
    print(f"åŸæ–‡: {original_en}")
    print(f"å‚è€ƒ: {reference_zh}")
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é£æ ¼ç±»å‹åˆ—è¡¨
    style_types_str = ", ".join(style_types)
    print(f"\né£æ ¼å‘é‡ (1Ã—4): [{style_types_str}]")
    
    translation_types = ["correct_style_correct_meaning", 
                        "correct_style_wrong_meaning",
                        "wrong_style_correct_meaning", 
                        "wrong_style_wrong_meaning"]
    
    for i, (style_reward, trans_type) in enumerate(zip(style_rewards, translation_types)):
        # ä»é£æ ¼å¥–åŠ±ç»“æœä¸­æå–ä¿¡æ¯
        similarity_score = style_reward['similarity_score']
        source_style = style_reward.get('source_main_style', 'unknown')
        target_style = style_reward.get('target_main_style', 'unknown')
        
        # è·å–è¯¦ç»†çš„é£æ ¼æ¦‚ç‡åˆ†å¸ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        source_probs = style_reward.get('style_details', {}).get('source_style_probs', [])
        target_probs = style_reward.get('style_details', {}).get('target_style_probs', [])
        
        print(f"ç¿»è¯‘ {i+1} ({trans_type}):")
        print(f"  ç›¸ä¼¼åº¦: {similarity_score:.4f}")
        print(f"  æºé£æ ¼: {source_style}")
        print(f"  ç›®æ ‡é£æ ¼: {target_style}")
        print(f"  é£æ ¼åŒ¹é…: {'âœ“' if style_reward.get('style_match', False) else 'âœ—'}")
        
        # å¦‚æœæœ‰é£æ ¼æ¦‚ç‡åˆ†å¸ƒï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if source_probs and len(source_probs) == len(style_types):
            print("  æºæ–‡æœ¬é£æ ¼æ¦‚ç‡:")
            for style, prob in zip(style_types, source_probs):
                print(f"    - {style}: {prob:.4f}")
        
        if target_probs and len(target_probs) == len(style_types):
            print("  ç›®æ ‡æ–‡æœ¬é£æ ¼æ¦‚ç‡:")
            for style, prob in zip(style_types, target_probs):
                print(f"    - {style}: {prob:.4f}")


def load_config(config_path: str) -> Dict[str, Any]:
    """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            'model': {'device': 'cpu'},
            'reward': {
                'test_mode': True,
                'style_types': ['law', 'science', 'news', 'literature'],
                'format_reward_weight': 0.2,
                'semantic_reward_weight': 0.7,
                'style_reward_weight': 0.1,
                'chinese_bert_path': 'mock_path',
                'english_bert_path': 'mock_path',
                'comet_model': 'wmt22-cometkiwi-da',
                'comet_device': 'cpu',
                'comet_path': None
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¼€å§‹å¥–åŠ±ä½“ç³»æµ‹è¯•")
    
    # ä»config.yamlåŠ è½½é…ç½®
    config = load_config("config.yaml")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ¨¡å¼
    if config['reward']['test_mode']:
        logger.info("å½“å‰ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿé£æ ¼å¥–åŠ±æ¨¡å‹")
    else:
        logger.info("å½“å‰ä¸ºæ­£å¼æ¨¡å¼ï¼Œå°†ä½¿ç”¨çœŸå®BERTé£æ ¼å¥–åŠ±æ¨¡å‹")
        logger.info(f"ä¸­æ–‡BERTæ¨¡å‹è·¯å¾„: {config['reward']['chinese_bert_path']}")
        logger.info(f"è‹±æ–‡BERTæ¨¡å‹è·¯å¾„: {config['reward']['english_bert_path']}")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    logger.info(f"å¥–åŠ±æƒé‡ - æ ¼å¼: {config['reward']['format_reward_weight']}, "
               f"è¯­ä¹‰: {config['reward']['semantic_reward_weight']}, "
               f"é£æ ¼: {config['reward']['style_reward_weight']}")
    logger.info(f"COMETæ¨¡å‹: {config['reward']['comet_model']}, "
               f"è®¾å¤‡: {config['reward']['comet_device']}")
    logger.info(f"é£æ ¼ç±»å‹: {', '.join(config['reward']['style_types'])}")
    
    # åˆå§‹åŒ–å¥–åŠ±ç®¡ç†å™¨
    reward_manager = RewardManager(config)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data("enzh_fake_trans.json")
    
    if not test_data:
        logger.error("æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
        return
    
    logger.info(f"æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
    
    # è¿›è¡Œå››è½®æµ‹è¯•
    for round_idx, test_item in enumerate(test_data):
        if round_idx >= 4:  # åªæµ‹è¯•å‰4è½®
            break
            
        original_en = test_item['en']
        reference_zh = test_item['zh']
        translations = test_item['translations']
        
        # åˆ›å»ºæ¨¡æ‹Ÿå“åº”
        mock_responses = create_mock_responses(original_en, translations)
        
        # åˆ›å»ºæç¤ºï¼ˆæ¨¡æ‹Ÿç¿»è¯‘ä»»åŠ¡ï¼‰
        prompts = [f"è¯·å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š{original_en}"] * 4
        
        # è®¾ç½®è¯­è¨€å¯¹
        language_pairs = ['en-zh'] * 4
        
        logger.info(f"\nå¼€å§‹ç¬¬ {round_idx + 1} è½®æµ‹è¯•...")
        
        try:
            # è®¡ç®—å¥–åŠ±
            reward_results = reward_manager.calculate_total_reward(
                generated_texts=mock_responses,
                source_texts=[original_en] * 4,
                prompts=prompts,
                language_pairs=language_pairs,
                reference_texts=[reference_zh] * 4  # ä½¿ç”¨å‚è€ƒè¯‘æ–‡è¿›è¡ŒCOMETè®¡ç®—
            )
            
            # æ‰“å°è¯¦ç»†å¥–åŠ±ä¿¡æ¯
            print_reward_details(round_idx, original_en, reference_zh, 
                               mock_responses, reward_results)
            
            # æ‰“å°é£æ ¼å‘é‡ä¿¡æ¯ï¼Œä¼ å…¥é…ç½®ä¸­çš„é£æ ¼ç±»å‹
            print_style_vectors(round_idx, original_en, reference_zh, 
                               reward_results['style_rewards'],
                               config['reward']['style_types'])
            
            logger.info(f"ç¬¬ {round_idx + 1} è½®æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç¬¬ {round_idx + 1} è½®æµ‹è¯•å¤±è´¥: {e}")
            continue
    
    logger.info("å¥–åŠ±ä½“ç³»æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()