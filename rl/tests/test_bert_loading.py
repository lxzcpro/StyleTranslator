#!/usr/bin/env python3
"""
ä½¿ç”¨ StyleDetector åŠ è½½å¹¶æµ‹è¯•ä¸­æ–‡ / è‹±æ–‡ BERT æ¨¡å‹
åŸºäº bert_test2.py çš„æ­£ç¡®åŠ è½½æ–¹å¼å®ç°
"""

import torch
from transformers import AutoTokenizer
from style_detector.model.model import StyleDetector  # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥æ–¹å¼
import yaml
import os


# ===========================
# é…ç½®æ–‡ä»¶åŠ è½½
# ===========================
def load_config(config_path='config.yaml'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None


def load_model(ckpt_path):
    """åŠ è½½æ¨¡å‹ï¼ˆåŸºäºbert_test2.pyçš„æ­£ç¡®å®ç°ï¼‰"""
    print(f"æ­£åœ¨ä»checkpointåŠ è½½æ¨¡å‹: {ckpt_path}")
    
    # ç›´æ¥ä½¿ç”¨ StyleDetector ç±»åŠ è½½ checkpoint
    model = StyleDetector.load_from_checkpoint(ckpt_path)
    
    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œè®¾å¤‡: {device}")
    return model


def predict_text(model, tokenizer, text):
    """æ¨¡å‹æ¨ç†å‡½æ•°ï¼ˆåŸºäºbert_test2.pyçš„å®ç°ï¼‰"""
    device = model.device
    
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=512,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        probabilities = torch.softmax(logits, dim=-1).squeeze()
        predicted_class = torch.argmax(probabilities).item()
    
    return predicted_class, probabilities.cpu().numpy()


# ===========================
# StyleDetector æ¨¡å‹æµ‹è¯•é€»è¾‘
# ===========================
def test_style_detector(model_path, model_name, style_types):
    """æµ‹è¯•StyleDetectoræ¨¡å‹"""
    print(f"\n{'=' * 60}")
    print(f"æµ‹è¯•æ¨¡å‹ï¼š{model_name}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"é£æ ¼ç±»åˆ«: {style_types}")
    print(f"{'=' * 60}")

    try:
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False

        # 1. åŠ è½½æ¨¡å‹
        model = load_model(model_path)
        
        # 2. åŠ è½½åˆ†è¯å™¨ï¼ˆä»æ¨¡å‹çš„è¶…å‚æ•°ä¸­è·å–æ¨¡å‹åç§°ï¼‰
        print("\næ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model.hparams.model_name)
            print(f"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ: {model.hparams.model_name}")
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†è¯å™¨å¤±è´¥: {e}")
            return False

        # 3. æµ‹è¯•è¾“å…¥æ–‡æœ¬
        if "ä¸­æ–‡" in model_name or "chinese" in model_name.lower():
            test_cases = [
                ("æ³•å¾‹é£æ ¼", "å½“äº‹äººæ‰¿è¯ºéµå®ˆæœ¬åˆåŒé¡¹ä¸‹çš„å…¨éƒ¨ä¹‰åŠ¡ï¼Œä»»ä½•è¿çº¦è¡Œä¸ºå‡è§†ä¸ºå®è´¨æ€§è¿åï¼Œå¹¶ä¾æ³•æ‰¿æ‹…ç›¸åº”æ³•å¾‹è´£ä»»ã€‚"),
                ("æ–°é—»é£æ ¼", "æ®æœ‰å…³éƒ¨é—¨é€éœ²ï¼Œä¸ºåº”å¯¹å®¢æµå¢é•¿ï¼ŒåŸå¸‚å…¬å…±äº¤é€šç³»ç»Ÿå°†äºä¸‹æœˆå¯åŠ¨æ–°ä¸€è½®å‡çº§æ”¹é€ ã€‚"),
                ("æ–‡å­¦é£æ ¼", "é»„æ˜çš„å…‰çº¿åœ¨è¡—å··é—´ç¼“ç¼“æµæ·Œï¼Œä»¿ä½›ä¸€å±‚è½»çº±ï¼Œä¸ºè¿™åº§é™è°§çš„åŸå¸‚æ·»ä¸Šäº†æ¸©æŸ”çš„è‰²å½©ã€‚"),
                ("ç§‘ç ”è®ºæ–‡é£æ ¼", "å®éªŒç»“æœè¡¨æ˜ï¼Œå°†å¤šæ¨¡æ€ç‰¹å¾èå…¥æ¨¡å‹ç»“æ„èƒ½å¤Ÿåœ¨å¤šé¡¹è¯„æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—æå‡ç³»ç»Ÿçš„ç¨³å¥æ€§ã€‚")
            ]
        else:
            test_cases = [
                ("æ³•å¾‹é£æ ¼", "The party hereby acknowledges that any breach of the obligations stipulated in this Agreement shall constitute a material violation subject to remedies permitted under applicable law."),
                ("æ–°é—»é£æ ¼", "According to officials, the city's public transportation system will undergo a major upgrade next month to address increasing commuter demand."),
                ("æ–‡å­¦é£æ ¼", "The dusk settled like a soft veil over the quiet town, and every fading ray of light seemed to breathe its own wistful farewell."),
                ("ç§‘ç ”è®ºæ–‡é£æ ¼", "Our findings demonstrate that integrating multi-modal features significantly improves the robustness of the proposed model across diverse evaluation benchmarks.")
            ]

        # 4. æ‰§è¡Œæ¨ç†
        print("\næµ‹è¯•æ¨¡å‹æ¨ç†...")
        for style_type, text in test_cases:
            predicted_class, probabilities = predict_text(model, tokenizer, text)
            predicted_style = style_types[predicted_class] if predicted_class < len(style_types) else f"æœªçŸ¥ç±»åˆ«{predicted_class}"
            
            print(f"\nã€{style_type}ã€‘")
            print(f"æ–‡æœ¬: '{text}'")
            print(f"é¢„æµ‹ç±»åˆ«: {predicted_class} ({predicted_style})")
            print("å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒï¼š")
            for i, (style, prob) in enumerate(zip(style_types, probabilities)):
                print(f"  - {style}: {prob:.8f}")
            print("-" * 60)

        print(f"\nğŸ‰ {model_name} æ¨¡å‹æµ‹è¯•å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ {model_name} æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ
        return False


# ===========================
# ä¸»å‡½æ•°
# ===========================
def main():
    """ä¸»å‡½æ•°"""
    print("BERT é£æ ¼åˆ†ç±»å™¨ï¼ˆStyleDetectorï¼‰æµ‹è¯•å¼€å§‹")
    print("=" * 60)

    # åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config()
    if not config:
        return

    # è·å–å¥–åŠ±é…ç½®
    reward_cfg = config.get("reward", {})
    
    # è·å–æ¨¡å‹è·¯å¾„å’Œé£æ ¼ç±»å‹
    zh_path = reward_cfg.get("chinese_bert_path", "")
    en_path = reward_cfg.get("english_bert_path", "")
    style_types = reward_cfg.get("style_types", [])

    print("è¯»å–é…ç½®ï¼š")
    print(f"- ä¸­æ–‡æ¨¡å‹è·¯å¾„: {zh_path}")
    print(f"- è‹±æ–‡æ¨¡å‹è·¯å¾„: {en_path}")
    print(f"- é£æ ¼ç±»åˆ«: {style_types}")

    # æµ‹è¯•ä¸­æ–‡æ¨¡å‹
    zh_ok = False
    if zh_path and os.path.exists(zh_path):
        zh_ok = test_style_detector(zh_path, "ä¸­æ–‡BERT (StyleDetector)", style_types)
    else:
        print(f"âŒ ä¸­æ–‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸ºç©º: {zh_path}")

    # æµ‹è¯•è‹±æ–‡æ¨¡å‹
    en_ok = False
    if en_path and os.path.exists(en_path):
        en_ok = test_style_detector(en_path, "è‹±æ–‡BERT (StyleDetector)", style_types)
    else:
        print(f"âŒ è‹±æ–‡æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸ºç©º: {en_path}")

    # è¾“å‡ºæµ‹è¯•ç»“æœæ€»ç»“
    print("\n====== æµ‹è¯•ç»“æœæ±‡æ€» ======")
    print(f"ä¸­æ–‡æ¨¡å‹: {'é€šè¿‡ âœ”' if zh_ok else 'å¤±è´¥ âœ˜'}")
    print(f"è‹±æ–‡æ¨¡å‹: {'é€šè¿‡ âœ”' if en_ok else 'å¤±è´¥ âœ˜'}")
    
    if zh_ok and en_ok:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼å¯ä»¥æ­£å¸¸ä½¿ç”¨çœŸå®BERTæ¨¡å‹è¿›è¡Œé£æ ¼å¥–åŠ±è®¡ç®—ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæ ¼å¼ã€‚")


if __name__ == "__main__":
    main()
